{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c791958c",
   "metadata": {},
   "source": [
    "### Crop images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc273fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown class: difficult — skipping\n",
      "Unknown class: difficult — skipping\n",
      "Unknown class: difficult — skipping\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Kunstig intelligens programmering/Prosjekt 1/.conda/lib/python3.11/site-packages/PIL/ImageFile.py:554\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     fh \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mfileno()\n\u001b[1;32m    555\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, label, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(save_path):\n\u001b[0;32m---> 46\u001b[0m     cropped\u001b[38;5;241m.\u001b[39msave(save_path)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists, skipping save.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Kunstig intelligens programmering/Prosjekt 1/.conda/lib/python3.11/site-packages/PIL/Image.py:2596\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2593\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2596\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m~/Kunstig intelligens programmering/Prosjekt 1/.conda/lib/python3.11/site-packages/PIL/PngImagePlugin.py:1488\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     single_im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[1;32m   1485\u001b[0m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[1;32m   1486\u001b[0m     )\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_im:\n\u001b[0;32m-> 1488\u001b[0m     ImageFile\u001b[38;5;241m.\u001b[39m_save(\n\u001b[1;32m   1489\u001b[0m         single_im,\n\u001b[1;32m   1490\u001b[0m         cast(IO[\u001b[38;5;28mbytes\u001b[39m], _idat(fp, chunk)),\n\u001b[1;32m   1491\u001b[0m         [ImageFile\u001b[38;5;241m.\u001b[39m_Tile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m single_im\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m0\u001b[39m, rawmode)],\n\u001b[1;32m   1492\u001b[0m     )\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m~/Kunstig intelligens programmering/Prosjekt 1/.conda/lib/python3.11/site-packages/PIL/ImageFile.py:558\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    556\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 558\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[38;5;28;01mNone\u001b[39;00m, exc)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    560\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/Kunstig intelligens programmering/Prosjekt 1/.conda/lib/python3.11/site-packages/PIL/ImageFile.py:584\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mencode(bufsize)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    585\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "images_path = \"../A-Dataset-and-Benchmark-for-Malaria-Life-Cycle-Classification-in-Thin-Blood-Smear-Images/IML_Malaria\"\n",
    "annotation_path = \"../A-Dataset-and-Benchmark-for-Malaria-Life-Cycle-Classification-in-Thin-Blood-Smear-Images/annotations.json\"\n",
    "output_dir = \"cell_dataset/\"\n",
    "\n",
    "# Make map for each class\n",
    "classes = [\"ring\", \"trophozoite\", \"schizont\", \"gametocyte\", \"red blood cell\"]\n",
    "for cls in classes:\n",
    "    os.makedirs(os.path.join(output_dir, cls), exist_ok=True)\n",
    "\n",
    "# Load annotations\n",
    "with open(annotation_path) as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "# Iterate through each image and its annotations\n",
    "for entry in ground_truth:\n",
    "    image_name = entry[\"image_name\"]\n",
    "    image_path = os.path.join(images_path, image_name)\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Bilde ikke funnet: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    for i, obj in enumerate(entry[\"objects\"]):\n",
    "        label = obj[\"type\"]  \n",
    "        if label not in classes:\n",
    "            print(f\"Unknown class: {label} — skipping\")\n",
    "            continue\n",
    "\n",
    "        # Bounding box\n",
    "        x = int(obj[\"bbox\"][\"x\"])\n",
    "        y = int(obj[\"bbox\"][\"y\"])\n",
    "        w = int(obj[\"bbox\"][\"w\"])\n",
    "        h = int(obj[\"bbox\"][\"h\"])\n",
    "\n",
    "        # Crop and save\n",
    "        cropped = image.crop((x, y, x + w, y + h))\n",
    "        save_path = os.path.join(output_dir, label, f\"{image_name[:-4]}_{i}.png\")\n",
    "        if not os.path.exists(save_path):\n",
    "            cropped.save(save_path)\n",
    "        else:\n",
    "            print(f\"File {save_path} already exists, skipping save.\")\n",
    "print(\" Done with cropping and saving images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d193bdd6",
   "metadata": {},
   "source": [
    "### Split data into train and valitadion set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29341ee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cell_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(val_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# List all class folders\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m classes \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(input_dir) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, d))]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplitting class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cell_dataset'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import shutil \n",
    "\n",
    "# Paths\n",
    "input_dir = \"cell_dataset\"        # Your full dataset with all images in class folders\n",
    "output_dir = \"data_split\"         # Where train/val folders will be created\n",
    "train_ratio = 0.8                 # 80% train, 20% validation split\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Create train and val directories\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "val_dir = os.path.join(output_dir, \"val\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# List all class folders\n",
    "classes = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"Splitting class: {cls}\")\n",
    "    cls_input_path = os.path.join(input_dir, cls)\n",
    "    files = os.listdir(cls_input_path)\n",
    "    random.shuffle(files)  # Shuffle files before splitting\n",
    "\n",
    "    train_count = int(len(files) * train_ratio)\n",
    "    train_files = files[:train_count]\n",
    "    val_files = files[train_count:]\n",
    "\n",
    "    # Create class folders inside train and val directories\n",
    "    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, cls), exist_ok=True)\n",
    "\n",
    "    # Copy training files\n",
    "    for f in train_files:\n",
    "        shutil.copy2(os.path.join(cls_input_path, f), os.path.join(train_dir, cls, f))\n",
    "\n",
    "    # Copy validation files\n",
    "    for f in val_files:\n",
    "        shutil.copy2(os.path.join(cls_input_path, f), os.path.join(val_dir, cls, f))\n",
    "\n",
    "print(\"Dataset split into train and validation sets!\")\n",
    "\n",
    "# After splitting, delete the original dataset folder to save space\n",
    "shutil.rmtree(\"cell_dataset\")\n",
    "print(\"'cell_dataset' folder has been deleted to save space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f850d",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Image transformations (augmentasjon for trening, bare normalisering for validering)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # Standard ImageNet mean/std\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Last inn datasett\n",
    "train_dataset = datasets.ImageFolder(\"data_split/train\", transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(\"data_split/val\", transform=val_transforms)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab77543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreakristiane/MalariaClassification/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/andreakristiane/MalariaClassification/.venv/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Training Epoch 1:  78%|███████▊  | 1497/1922 [1:58:08<41:26,  5.85s/it]    "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# === 1. Paths and constants ===\n",
    "data_dir = \"data_split\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 5\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "\n",
    "# === 2. Load pretrained model and feature extractor ===\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=num_classes\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# === 3. Custom dataset class ===\n",
    "class MalariaDataset(Dataset):\n",
    "    def __init__(self, root_dir, feature_extractor):\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.paths = []\n",
    "        self.labels = []\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            cls_folder = os.path.join(root_dir, cls)\n",
    "            for img_name in os.listdir(cls_folder):\n",
    "                self.paths.append(os.path.join(cls_folder, img_name))\n",
    "                self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].squeeze()  # Remove batch dim\n",
    "        label = self.labels[idx]\n",
    "        return pixel_values, label\n",
    "\n",
    "# === 4. Load data ===\n",
    "train_dataset = MalariaDataset(train_dir, feature_extractor)\n",
    "val_dataset = MalariaDataset(val_dir, feature_extractor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# === 5. Optimizer and loss ===\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# === 6. Training loop ===\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        pixel_values, labels = batch\n",
    "        pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {total_loss/len(train_loader):.4f} - Train Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # === 7. Evaluation ===\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            pixel_values, labels = batch\n",
    "            pixel_values, labels = pixel_values.to(device), labels.to(device)\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    print(f\"Epoch {epoch+1} - Validation Accuracy: {val_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
